{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')  # stopwords data.\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups # 이런 자료가 내장.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>Well i m not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>Yeah  do you expect people to read the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>Well  I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>Danny Rubenstein  an Israeli journalist  will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>I agree   Home runs off Clemens are always me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>^^^^^^ N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      Well i m not sure about the story nad it did s...  \n",
       "1             Yeah  do you expect people to read the ...  \n",
       "2      Although I realize that principle is not one o...  \n",
       "3      Notwithstanding all the legitimate fuss about ...  \n",
       "4      Well  I will have to change the scoring on my ...  \n",
       "...                                                  ...  \n",
       "11309  Danny Rubenstein  an Israeli journalist  will ...  \n",
       "11310                                                     \n",
       "11311   I agree   Home runs off Clemens are always me...  \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...  \n",
       "11313                                        ^^^^^^ N...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 다운로드 및 전처리하기\n",
    "def get_news() :\n",
    "    #20newsgroup 다운로드\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove = ('headers','footers','quotes'))\n",
    "    documents = dataset.data\n",
    "    \n",
    "    news_df = pd.DataFrame({'document' : documents})\n",
    "    # 전처리\n",
    "    news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-z]\", \" \")  #특수 문자 제거\n",
    "    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : ' '.join([w for w in x.split() if len(w) > 3 ]))\n",
    "    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x:x.lower())    # 전체 단어에 대해서 소문자 변환!\n",
    "    tokenized_doc  = news_df['clean_doc'].apply(lambda x : x.split()) #토큰화함. \n",
    "    \n",
    "    stop_words = stopwords.words('english') # NLTK 불용어 조회\n",
    "    \n",
    "\n",
    "    return tokenized_doc.apply(lambda x : ' '.join([item for item in x if item not in stop_words]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure story seem biased disagree statement...\n",
       "1        yeah expect people read actually accept hard a...\n",
       "2        although realize principle strongest points wo...\n",
       "3        notwithstanding legitimate fuss proposal much ...\n",
       "4        well change scoring playoff pool unfortunately...\n",
       "                               ...                        \n",
       "11309    danny rubenstein israeli journalist speaking t...\n",
       "11310                                                     \n",
       "11311    agree home runs clemens always memorable kinda...\n",
       "11312    used deskjet orange micros grappler system upd...\n",
       "11313    ^^^^^^ argument murphy scared hell came last y...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백으로 토큰 분리\n",
    "def my_tokenizer(text) :\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# def my_tokenizer\n",
    "# text.split()\n",
    "# SPLIT아니고 mecab정의해놓으면 한글도 사용 가능\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer = my_tokenizer)\n",
    "tfidf = tfidf_vect.fit_transform(tokenized_docs)\n",
    "\n",
    "# 모델 선언.\n",
    "lda = LatentDirichletAllocation(n_components = 20 )\n",
    "lda_output = lda.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06483471, 0.00648184, 0.00648184, ..., 0.00648185, 0.00648185,\n",
       "        0.00648184],\n",
       "       [0.00748012, 0.00748012, 0.00748012, ..., 0.06987639, 0.00748012,\n",
       "        0.00748012],\n",
       "       [0.00711544, 0.00711544, 0.00711544, ..., 0.00711544, 0.00711544,\n",
       "        0.00711544],\n",
       "       ...,\n",
       "       [0.0117179 , 0.0117179 , 0.0117179 , ..., 0.0117179 , 0.0117179 ,\n",
       "        0.0117179 ],\n",
       "       [0.01312944, 0.01312943, 0.01312943, ..., 0.01312943, 0.01312943,\n",
       "        0.01312943],\n",
       "       [0.00552023, 0.00552023, 0.00552023, ..., 0.00552023, 0.00552023,\n",
       "        0.00552023]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
